{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "\n",
    "*Please fill out the relevant cells below according to the instructions. When done, save the notebook and export it to PDF, upload both the `ipynb` and the PDF file to Canvas.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group Members\n",
    "\n",
    "*Group submission is highly encouraged. If you submit as part of group, list all group members here. Groups can comprise up to 4 students.*\n",
    "\n",
    "* Adam Applegate\n",
    "* Beatrix Brahms\n",
    "* \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparse Interactions\n",
    "\n",
    "### Preparation (3pts)\n",
    "\n",
    "Review the paper [The Kernel Interaction Trick: Fast Bayesian Discovery of Pairwise Interactions in High Dimensions](https://arxiv.org/abs/1905.06501) by Agrawal et al. (2019). Start with the general concepts and then go into the finer details.\n",
    "\n",
    "When you feel comfortable with the content, answer the following questions:\n",
    "\n",
    "1. Why does the Gaussian scale mixture prior promote sparsity of the regression coefficients $\\theta$?\n",
    "2. What are the required properties of the model in Eq. (3) that allow it to be rewritten in the form of Eq. (6)?\n",
    "3. What are the conceptual and practical limitation of the approach?\n",
    "\n",
    "**Hint:** Some of the answers may require parsing the relevant references."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.\n",
    "\"See sections 4.1-4.3 in paper:In order to promote sparsity in the main effects, we require two ingredients: (1) a prior on the global shrinkage parameter η1 and (2) a prior on the local shrinkage parameters contained in κ ∈ Rp, Conditional on η1 and κ (Carvalho et al. (2009); Piironen & Vehtari (2017).)\n",
    "\"maybe this ref is helpful: Griffin, J. and Brown, P. Hierarchical shrinkage priors for regression models. Bayesian Analysis, 12: 135–159, 2017\"\n",
    "\n",
    "2.\n",
    "\n",
    "3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code adaptation (2pts)\n",
    "\n",
    "The method SKIM from the paper's section 6 has been implemented in jax/Numpyro [here](https://pyro.ai/numpyro/examples/sparse_regression.html). Review the code and recognize how the theoretical concepts of the Kernel Interaction Trick and the specific features of SKIM have been implemented. Then copy the code to this notebook and modify it so that you can execute the provided test example inline. Confirm that you get a result comparable to theirs.\n",
    "\n",
    "The last step of their example analysis (sampling from the posterior with the method `sample_theta_space`) often returns `nan`s. It also reports the posterior for all $\\theta$ (active and inactive ones), and only for one sample at a time. That's really clunky. Modify this function to produce valid posterior samples of $\\theta$ from all the $\\tau$ samples from the MCMC step, but restrict yourself to the active direct and pairwise interaction terms. Visualize the posterior from the example with `corner`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import itertools\n",
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from jax import vmap\n",
    "import jax.numpy as jnp\n",
    "import jax.random as random\n",
    "from jax.scipy.linalg import cho_factor, cho_solve, solve_triangular\n",
    "\n",
    "import numpyro\n",
    "import numpyro.distributions as dist\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "\n",
    "import corner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sample: 100%|██████████| 1500/1500 [01:23<00:00, 17.89it/s, 15 steps of size 1.77e-01. acc. prob=0.88]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "                mean       std    median      5.0%     95.0%     n_eff     r_hat\n",
      "      eta1      0.00      0.00      0.00      0.00      0.00    382.93      1.00\n",
      " lambda[0]   2667.13   7673.19    858.44     68.01   4913.02    623.58      1.00\n",
      " lambda[1]   4813.62  22046.76   1161.62     66.84   6606.99    386.81      1.01\n",
      " lambda[2]    467.17   1987.94    193.31     25.98    614.06    205.98      1.01\n",
      " lambda[3]      1.22      1.73      0.71      0.00      2.92    800.63      1.00\n",
      " lambda[4]      1.31      2.26      0.75      0.01      2.67    756.16      1.00\n",
      " lambda[5]      1.23      1.63      0.73      0.00      2.91    904.83      1.00\n",
      " lambda[6]      1.85      2.64      1.03      0.00      4.29    490.63      1.00\n",
      " lambda[7]      1.46      2.68      0.87      0.00      3.11    792.27      1.00\n",
      " lambda[8]      1.11      1.58      0.66      0.00      2.43    743.63      1.00\n",
      " lambda[9]      1.47      2.13      0.81      0.01      3.49   1050.65      1.00\n",
      "lambda[10]      1.31      1.79      0.79      0.00      3.02    912.26      1.00\n",
      "lambda[11]      2.21      3.08      1.35      0.00      4.91    884.10      1.00\n",
      "lambda[12]      1.38      2.04      0.77      0.00      3.02   1063.64      1.00\n",
      "lambda[13]      1.17      1.64      0.73      0.00      2.63   1025.82      1.00\n",
      "lambda[14]      1.00      1.29      0.62      0.01      2.19   1021.38      1.00\n",
      "lambda[15]      1.72      4.66      0.79      0.00      3.26    551.12      1.00\n",
      "lambda[16]      3.35      5.47      1.68      0.00      7.21    722.85      1.00\n",
      "lambda[17]      1.18      1.56      0.76      0.01      2.55    963.67      1.00\n",
      "lambda[18]      1.21      1.65      0.72      0.00      2.69    845.50      1.00\n",
      "lambda[19]      1.34      1.74      0.82      0.00      2.93    859.61      1.00\n",
      "       msq      0.78      0.51      0.66      0.22      1.39    714.53      1.00\n",
      "     sigma      0.02      0.00      0.02      0.02      0.02    539.08      1.01\n",
      "      xisq      0.37      0.23      0.31      0.11      0.61    601.53      1.00\n",
      "\n",
      "Number of divergences: 0\n",
      "\n",
      "MCMC elapsed time: 91.2925443649292\n",
      "Coefficients theta_1 to theta_3 used to generate the data: [0.69618857 0.7082517  0.35156995]\n",
      "The single quadratic coefficient theta_{1,2} used to generate the data: 0.4637312\n",
      "[dimension 01/20]  active:\t6.88e-01 +- nan\n",
      "[dimension 02/20]  active:\t7.06e-01 +- 2.18e-02\n",
      "[dimension 03/20]  active:\t3.53e-01 +- 6.92e-03\n",
      "[dimension 04/20]  inactive:\t2.51e-04 +- 7.16e-03\n",
      "[dimension 05/20]  inactive:\t-4.02e-04 +- 7.19e-03\n",
      "[dimension 06/20]  inactive:\t-2.08e-04 +- 7.16e-03\n",
      "[dimension 07/20]  inactive:\t1.46e-03 +- 7.37e-03\n",
      "[dimension 08/20]  inactive:\t-8.76e-04 +- 7.23e-03\n",
      "[dimension 09/20]  inactive:\t-4.58e-04 +- 7.18e-03\n",
      "[dimension 10/20]  inactive:\t-2.31e-04 +- 7.20e-03\n",
      "[dimension 11/20]  inactive:\t6.08e-04 +- 7.22e-03\n",
      "[dimension 12/20]  inactive:\t-1.95e-03 +- 7.39e-03\n",
      "[dimension 13/20]  inactive:\t7.38e-04 +- 7.23e-03\n",
      "[dimension 14/20]  inactive:\t1.51e-04 +- 7.17e-03\n",
      "[dimension 15/20]  inactive:\t1.57e-04 +- 7.15e-03\n",
      "[dimension 16/20]  inactive:\t3.43e-04 +- 7.21e-03\n",
      "[dimension 17/20]  inactive:\t-2.45e-03 +- 7.57e-03\n",
      "[dimension 18/20]  inactive:\t-5.61e-05 +- 7.17e-03\n",
      "[dimension 19/20]  inactive:\t3.51e-04 +- 7.17e-03\n",
      "[dimension 20/20]  inactive:\t5.76e-04 +- 7.22e-03\n",
      "Identified a total of 3 active dimensions; expected 3.\n",
      "Identified pairwise interaction between dimensions 1 and 2: 4.56e-01 +- 6.04e-03\n",
      "Single posterior sample theta:\n",
      " [ 0.6651296   0.69244725  0.36388922 -0.001398    0.01136595 -0.00132565\n",
      "  0.00539337 -0.0094489  -0.00303606  0.00278337  0.01728954 -0.00783229\n",
      "  0.00457164 -0.00500625 -0.01433851 -0.00274551 -0.00337468 -0.00693334\n",
      " -0.00487604  0.00250014  0.46028137  0.00070566 -0.01325113]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def dot(X, Z):\n",
    "    return jnp.dot(X, Z[..., None])[..., 0]\n",
    "\n",
    "\n",
    "# The kernel that corresponds to our quadratic regressor.\n",
    "def kernel(X, Z, eta1, eta2, c, jitter=1.0e-4):\n",
    "    eta1sq = jnp.square(eta1)\n",
    "    eta2sq = jnp.square(eta2)\n",
    "    k1 = 0.5 * eta2sq * jnp.square(1.0 + dot(X, Z))\n",
    "    k2 = -0.5 * eta2sq * dot(jnp.square(X), jnp.square(Z))\n",
    "    k3 = (eta1sq - eta2sq) * dot(X, Z)\n",
    "    k4 = jnp.square(c) - 0.5 * eta2sq\n",
    "    if X.shape == Z.shape:\n",
    "        k4 += jitter * jnp.eye(X.shape[0])\n",
    "    return k1 + k2 + k3 + k4\n",
    "\n",
    "\n",
    "# Most of the model code is concerned with constructing the sparsity inducing prior.\n",
    "def model(X, Y, hypers):\n",
    "    S, P, N = hypers[\"expected_sparsity\"], X.shape[1], X.shape[0]\n",
    "\n",
    "    sigma = numpyro.sample(\"sigma\", dist.HalfNormal(hypers[\"alpha3\"]))\n",
    "    phi = sigma * (S / jnp.sqrt(N)) / (P - S)\n",
    "    eta1 = numpyro.sample(\"eta1\", dist.HalfCauchy(phi))\n",
    "\n",
    "    msq = numpyro.sample(\"msq\", dist.InverseGamma(hypers[\"alpha1\"], hypers[\"beta1\"]))\n",
    "    xisq = numpyro.sample(\"xisq\", dist.InverseGamma(hypers[\"alpha2\"], hypers[\"beta2\"]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "\n",
    "    lam = numpyro.sample(\"lambda\", dist.HalfCauchy(jnp.ones(P)))\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    # compute kernel\n",
    "    kX = kappa * X\n",
    "    k = kernel(kX, kX, eta1, eta2, hypers[\"c\"]) + sigma**2 * jnp.eye(N)\n",
    "    assert k.shape == (N, N)\n",
    "\n",
    "    # sample Y according to the standard gaussian process formula\n",
    "    numpyro.sample(\n",
    "        \"Y\",\n",
    "        dist.MultivariateNormal(loc=jnp.zeros(X.shape[0]), covariance_matrix=k),\n",
    "        obs=Y,\n",
    "    )\n",
    "\n",
    "\n",
    "# Compute the mean and variance of coefficient theta_i (where i = dimension) for a\n",
    "# MCMC sample of the kernel hyperparameters (eta1, xisq, ...).\n",
    "# Compare to theorem 5.1 in reference [1].\n",
    "def compute_singleton_mean_variance(X, Y, dimension, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N = X.shape[1], X.shape[0]\n",
    "\n",
    "    probe = jnp.zeros((2, P))\n",
    "    probe = probe.at[:, dimension].set(jnp.array([1.0, -1.0]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    k_xx_inv = jnp.linalg.inv(k_xx)\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    vec = jnp.array([0.50, -0.50])\n",
    "    mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))\n",
    "    mu = jnp.dot(mu, vec)\n",
    "\n",
    "    var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))\n",
    "    var = jnp.matmul(var, vec)\n",
    "    var = jnp.dot(var, vec)\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "# Compute the mean and variance of coefficient theta_ij for a MCMC sample of the\n",
    "# kernel hyperparameters (eta1, xisq, ...). Compare to theorem 5.1 in reference [1].\n",
    "def compute_pairwise_mean_variance(X, Y, dim1, dim2, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N = X.shape[1], X.shape[0]\n",
    "\n",
    "    probe = jnp.zeros((4, P))\n",
    "    probe = probe.at[:, dim1].set(jnp.array([1.0, 1.0, -1.0, -1.0]))\n",
    "    probe = probe.at[:, dim2].set(jnp.array([1.0, -1.0, 1.0, -1.0]))\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    k_xx_inv = jnp.linalg.inv(k_xx)\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    vec = jnp.array([0.25, -0.25, -0.25, 0.25])\n",
    "    mu = jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, Y))\n",
    "    mu = jnp.dot(mu, vec)\n",
    "\n",
    "    var = k_prbprb - jnp.matmul(k_probeX, jnp.matmul(k_xx_inv, jnp.transpose(k_probeX)))\n",
    "    var = jnp.matmul(var, vec)\n",
    "    var = jnp.dot(var, vec)\n",
    "\n",
    "    return mu, var\n",
    "\n",
    "\n",
    "# Sample coefficients theta from the posterior for a given MCMC sample.\n",
    "# The first P returned values are {theta_1, theta_2, ...., theta_P}, while\n",
    "# the remaining values are {theta_ij} for i,j in the list `active_dims`,\n",
    "# sorted so that i < j.\n",
    "\n",
    "#MODIFY FOR FINAL PROJECT\n",
    "# Modify this function to produce valid posterior samples of theta from all the tau samples from the MCMC step\n",
    "# but restrict yourself to the active direct and pairwise interaction terms\n",
    "def sample_theta_space(X, Y, active_dims, msq, lam, eta1, xisq, c, sigma):\n",
    "    P, N, M = X.shape[1], X.shape[0], len(active_dims)\n",
    "    # the total number of coefficients we return\n",
    "    num_coefficients = P + M * (M - 1) // 2\n",
    "\n",
    "    probe = jnp.zeros((2 * P + 2 * M * (M - 1), P))\n",
    "    vec = jnp.zeros((num_coefficients, 2 * P + 2 * M * (M - 1)))\n",
    "    start1 = 0\n",
    "    start2 = 0\n",
    "\n",
    "    for dim in range(P):\n",
    "        probe = probe.at[start1 : start1 + 2, dim].set(jnp.array([1.0, -1.0]))\n",
    "        vec = vec.at[start2, start1 : start1 + 2].set(jnp.array([0.5, -0.5]))\n",
    "        start1 += 2\n",
    "        start2 += 1\n",
    "\n",
    "    for dim1 in active_dims:\n",
    "        for dim2 in active_dims:\n",
    "            if dim1 >= dim2:\n",
    "                continue\n",
    "            probe = probe.at[start1 : start1 + 4, dim1].set(\n",
    "                jnp.array([1.0, 1.0, -1.0, -1.0])\n",
    "            )\n",
    "            probe = probe.at[start1 : start1 + 4, dim2].set(\n",
    "                jnp.array([1.0, -1.0, 1.0, -1.0])\n",
    "            )\n",
    "            vec = vec.at[start2, start1 : start1 + 4].set(\n",
    "                jnp.array([0.25, -0.25, -0.25, 0.25])\n",
    "            )\n",
    "            start1 += 4\n",
    "            start2 += 1\n",
    "\n",
    "    eta2 = jnp.square(eta1) * jnp.sqrt(xisq) / msq\n",
    "    kappa = jnp.sqrt(msq) * lam / jnp.sqrt(msq + jnp.square(eta1 * lam))\n",
    "\n",
    "    kX = kappa * X\n",
    "    kprobe = kappa * probe\n",
    "\n",
    "    k_xx = kernel(kX, kX, eta1, eta2, c) + sigma**2 * jnp.eye(N)\n",
    "    L = cho_factor(k_xx, lower=True)[0]\n",
    "    k_probeX = kernel(kprobe, kX, eta1, eta2, c)\n",
    "    k_prbprb = kernel(kprobe, kprobe, eta1, eta2, c)\n",
    "\n",
    "    mu = jnp.matmul(k_probeX, cho_solve((L, True), Y))\n",
    "    mu = jnp.sum(mu * vec, axis=-1)\n",
    "\n",
    "    Linv_k_probeX = solve_triangular(L, jnp.transpose(k_probeX), lower=True)\n",
    "    covar = k_prbprb - jnp.matmul(jnp.transpose(Linv_k_probeX), Linv_k_probeX)\n",
    "    covar = jnp.matmul(vec, jnp.matmul(covar, jnp.transpose(vec)))\n",
    "\n",
    "    # sample from N(mu, covar)\n",
    "    L = jnp.linalg.cholesky(covar)\n",
    "    sample = mu + jnp.matmul(L, np.random.randn(num_coefficients))\n",
    "\n",
    "    return sample\n",
    "\n",
    "\n",
    "# Helper function for doing HMC inference\n",
    "def run_inference(model, args, rng_key, X, Y, hypers):\n",
    "    start = time.time()\n",
    "    kernel = NUTS(model)\n",
    "    mcmc = MCMC(\n",
    "        kernel,\n",
    "        num_warmup=args.num_warmup,\n",
    "        num_samples=args.num_samples,\n",
    "        num_chains=args.num_chains,\n",
    "        progress_bar=False if \"NUMPYRO_SPHINXBUILD\" in os.environ else True,\n",
    "    )\n",
    "    mcmc.run(rng_key, X, Y, hypers)\n",
    "    mcmc.print_summary()\n",
    "    print(\"\\nMCMC elapsed time:\", time.time() - start)\n",
    "    return mcmc.get_samples()\n",
    "\n",
    "\n",
    "# Get the mean and variance of a gaussian mixture\n",
    "def gaussian_mixture_stats(mus, variances):\n",
    "    mean_mu = jnp.mean(mus)\n",
    "    mean_var = jnp.mean(variances) + jnp.mean(jnp.square(mus)) - jnp.square(mean_mu)\n",
    "    return mean_mu, mean_var\n",
    "\n",
    "\n",
    "# Create artificial regression dataset where only S out of P feature\n",
    "# dimensions contain signal and where there is a single pairwise interaction\n",
    "# between the first and second dimensions.\n",
    "def get_data(N=20, S=2, P=10, sigma_obs=0.05):\n",
    "    assert S < P and P > 1 and S > 0\n",
    "    np.random.seed(0)\n",
    "\n",
    "    X = np.random.randn(N, P)\n",
    "    # generate S coefficients with non-negligible magnitude\n",
    "    W = 0.5 + 2.5 * np.random.rand(S)\n",
    "    # generate data using the S coefficients and a single pairwise interaction\n",
    "    Y = (\n",
    "        np.sum(X[:, 0:S] * W, axis=-1)\n",
    "        + X[:, 0] * X[:, 1]\n",
    "        + sigma_obs * np.random.randn(N)\n",
    "    )\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y_std = jnp.std(Y)\n",
    "\n",
    "    assert X.shape == (N, P)\n",
    "    assert Y.shape == (N,)\n",
    "\n",
    "    return X, Y / Y_std, W / Y_std, 1.0 / Y_std\n",
    "\n",
    "\n",
    "# Helper function for analyzing the posterior statistics for coefficient theta_i\n",
    "def analyze_dimension(samples, X, Y, dimension, hypers):\n",
    "    vmap_args = (\n",
    "        samples[\"msq\"],\n",
    "        samples[\"lambda\"],\n",
    "        samples[\"eta1\"],\n",
    "        samples[\"xisq\"],\n",
    "        samples[\"sigma\"],\n",
    "    )\n",
    "    mus, variances = vmap(\n",
    "        lambda msq, lam, eta1, xisq, sigma: compute_singleton_mean_variance(\n",
    "            X, Y, dimension, msq, lam, eta1, xisq, hypers[\"c\"], sigma\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    mean, variance = gaussian_mixture_stats(mus, variances)\n",
    "    std = jnp.sqrt(variance)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "# Helper function for analyzing the posterior statistics for coefficient theta_ij\n",
    "def analyze_pair_of_dimensions(samples, X, Y, dim1, dim2, hypers):\n",
    "    vmap_args = (\n",
    "        samples[\"msq\"],\n",
    "        samples[\"lambda\"],\n",
    "        samples[\"eta1\"],\n",
    "        samples[\"xisq\"],\n",
    "        samples[\"sigma\"],\n",
    "    )\n",
    "    mus, variances = vmap(\n",
    "        lambda msq, lam, eta1, xisq, sigma: compute_pairwise_mean_variance(\n",
    "            X, Y, dim1, dim2, msq, lam, eta1, xisq, hypers[\"c\"], sigma\n",
    "        )\n",
    "    )(*vmap_args)\n",
    "    mean, variance = gaussian_mixture_stats(mus, variances)\n",
    "    std = jnp.sqrt(variance)\n",
    "    return mean, std\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    X, Y, expected_thetas, expected_pairwise = get_data(\n",
    "        N=args.num_data, P=args.num_dimensions, S=args.active_dimensions\n",
    "    )\n",
    "\n",
    "    # setup hyperparameters\n",
    "    hypers = {\n",
    "        \"expected_sparsity\": max(1.0, args.num_dimensions / 10),\n",
    "        \"alpha1\": 3.0,\n",
    "        \"beta1\": 1.0,\n",
    "        \"alpha2\": 3.0,\n",
    "        \"beta2\": 1.0,\n",
    "        \"alpha3\": 1.0,\n",
    "        \"c\": 1.0,\n",
    "    }\n",
    "\n",
    "    # do inference\n",
    "    rng_key = random.PRNGKey(0)\n",
    "    samples = run_inference(model, args, rng_key, X, Y, hypers)\n",
    "\n",
    "    # compute the mean and square root variance of each coefficient theta_i\n",
    "    means, stds = vmap(lambda dim: analyze_dimension(samples, X, Y, dim, hypers))(\n",
    "        jnp.arange(args.num_dimensions)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Coefficients theta_1 to theta_%d used to generate the data:\"\n",
    "        % args.active_dimensions,\n",
    "        expected_thetas,\n",
    "    )\n",
    "    print(\n",
    "        \"The single quadratic coefficient theta_{1,2} used to generate the data:\",\n",
    "        expected_pairwise,\n",
    "    )\n",
    "    active_dimensions = []\n",
    "\n",
    "    for dim, (mean, std) in enumerate(zip(means, stds)):\n",
    "        # we mark the dimension as inactive if the interval [mean - 3 * std, mean + 3 * std] contains zero\n",
    "        lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "        inactive = \"inactive\" if lower < 0.0 and upper > 0.0 else \"active\"\n",
    "        if inactive == \"active\":\n",
    "            active_dimensions.append(dim)\n",
    "        print(\n",
    "            \"[dimension %02d/%02d]  %s:\\t%.2e +- %.2e\"\n",
    "            % (dim + 1, args.num_dimensions, inactive, mean, std)\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Identified a total of %d active dimensions; expected %d.\"\n",
    "        % (len(active_dimensions), args.active_dimensions)\n",
    "    )\n",
    "\n",
    "    # Compute the mean and square root variance of coefficients theta_ij for i,j active dimensions.\n",
    "    # Note that the resulting numbers are only meaningful for i != j.\n",
    "    if len(active_dimensions) > 0:\n",
    "        dim_pairs = jnp.array(\n",
    "            list(itertools.product(active_dimensions, active_dimensions))\n",
    "        )\n",
    "        means, stds = vmap(\n",
    "            lambda dim_pair: analyze_pair_of_dimensions(\n",
    "                samples, X, Y, dim_pair[0], dim_pair[1], hypers\n",
    "            )\n",
    "        )(dim_pairs)\n",
    "        for dim_pair, mean, std in zip(dim_pairs, means, stds):\n",
    "            dim1, dim2 = dim_pair\n",
    "            if dim1 >= dim2:\n",
    "                continue\n",
    "            lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "            if not (lower < 0.0 and upper > 0.0):\n",
    "                format_str = \"Identified pairwise interaction between dimensions %d and %d: %.2e +- %.2e\"\n",
    "                print(format_str % (dim1 + 1, dim2 + 1, mean, std))\n",
    "\n",
    "        # Draw a single sample of coefficients theta from the posterior, where we return all singleton\n",
    "        # coefficients theta_i and pairwise coefficients theta_ij for i, j active dimensions. We use the\n",
    "        # final MCMC sample obtained from the HMC sampler.\n",
    "\n",
    "        # FOR THE PROJECT\n",
    "        # Modify this function to produce valid posterior samples of theta from all the tau samples from the MCMC step\n",
    "        # but restrict yourself to the active direct and pairwise interaction terms\n",
    "        thetas = sample_theta_space(\n",
    "            X,\n",
    "            Y,\n",
    "            active_dimensions,\n",
    "            samples[\"msq\"][-1],\n",
    "            samples[\"lambda\"][-1],\n",
    "            samples[\"eta1\"][-1],\n",
    "            samples[\"xisq\"][-1],\n",
    "            hypers[\"c\"],\n",
    "            samples[\"sigma\"][-1],\n",
    "        )\n",
    "        print(\"Single posterior sample theta:\\n\", thetas)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    assert numpyro.__version__.startswith(\"0.11.0\")\n",
    "    parser = argparse.ArgumentParser(description=\"Gaussian Process example\")\n",
    "    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n",
    "    parser.add_argument(\"--num-warmup\", nargs=\"?\", default=500, type=int)\n",
    "    parser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int)\n",
    "    parser.add_argument(\"--num-data\", nargs=\"?\", default=100, type=int)\n",
    "    parser.add_argument(\"--num-dimensions\", nargs=\"?\", default=20, type=int)\n",
    "    parser.add_argument(\"--active-dimensions\", nargs=\"?\", default=3, type=int)\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".')\n",
    "    #ipy seems to want a \"-f\" arg, so this is my fix atm\n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    numpyro.set_platform(args.device)\n",
    "    numpyro.set_host_device_count(args.num_chains)\n",
    "\n",
    "    main(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Copied from above for reference:**\n",
    "The last step of their example analysis (sampling from the posterior with the method `sample_theta_space`) often returns `nan`s. It also reports the posterior for all $\\theta$ (active and inactive ones), and only for one sample at a time. That's really clunky. Modify this function to produce valid posterior samples of $\\theta$ from all the $\\tau$ samples from the MCMC step, but restrict yourself to the active direct and pairwise interaction terms. \n",
    "\n",
    "**Visualize the posterior from the example with `corner`.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Application (5pts)\n",
    "\n",
    "Find an application from your area of research where the kernel-interaction method is directly applicable, or could be applied with some modification. Describe the application for a statistically knowledgeable but non-expert audience (think: your peers in SML 505). In particular, explain why the sparse interaction ansatz is justified. Then demonstrate the use with a suitable data set of your own choice. Explain what you find.\n",
    "\n",
    "This task has three parts:\n",
    "\n",
    "* Identify and descuss which possible effects there could be.\n",
    "* Find suitable data.\n",
    "* Perform the inference and interpret the results.\n",
    "\n",
    "You will probably need to iterate and refine along the way. Explain your reasoning about the kinds of features you decided to include in your analysis. Then report the most important direct and pairwise interactions. Visualized the posterior samples with `corner`.\n",
    "\n",
    "**Note:** This is an exploratory study. If your approach is sound, but the data don't show firm trends, points will be awarded. Make sure that you have permission to use the data and include it as separate file in your submission.\n",
    "\n",
    "**Hint:** Don't forget to standardize the data by subtracting the mean and dividing by the standard deviation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEW SINGLE CELL DATA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "from pynwb import NWBHDF5IO\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_path):\n",
    "    data = {}\n",
    "    nwb = NWBHDF5IO(data_path, mode='r').read()\n",
    "    trial_info = nwb.intervals['trials']\n",
    "    data['subj'] = nwb.subject.subject_id\n",
    "    data['description'] = nwb.experiment_description\n",
    "    data['rois'] = np.asarray(nwb.electrodes['location'].data)\n",
    "    data['confidence'] = np.array(trial_info.response_value.data)\n",
    "    data['condition'] = np.array(trial_info.stim_phase)\n",
    "    data['category'] = np.array(trial_info.stimCategory.data)\n",
    "    data['onset'] = np.array(trial_info.start_time.data)\n",
    "    data['recog_novelty'] = np.array(trial_info.new_old_labels_recog)\n",
    "    data['unit_electrodes'] = np.array(nwb.units['electrodes'].data)\n",
    "    # Convert the spike times to stimulus spike counts.\n",
    "    n_units = data['unit_electrodes'].shape[0]\n",
    "    n_stimuli = data['category'].shape[0]\n",
    "    spike_counts = np.zeros([n_stimuli, n_units])\n",
    "    for i in range(n_units):\n",
    "        spks = nwb.units.get_unit_spike_times(i)\n",
    "        spike_counts[:,i] = np.array([((spks>onset)&(spks<onset+1)).sum() for onset in data['onset']+.2])\n",
    "    data['spike_counts'] = zscore(spike_counts, 0)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD STUFF FOR HYPOTHALAMUS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def open_sessions(path):\n",
    "    with open(path, 'rb') as handle:\n",
    "        feats = pickle.load(handle)\n",
    "    return feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#giving it a shot with the multiregion photometry data\n",
    "big_data = open_sessions('traces_with_labels.pickle')\n",
    "test_feat = big_data['3095_d1_balbc_t1']\n",
    "\n",
    "# testing with 20 samples from one animal\n",
    "indices = np.linspace(0,len(test_feat)-1,2000, dtype=int)\n",
    "test_data = test_feat.loc[indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PrL (E)</th>\n",
       "      <th>PrL (I)</th>\n",
       "      <th>vLS (E)</th>\n",
       "      <th>vLS (I)</th>\n",
       "      <th>POA (E)</th>\n",
       "      <th>POA (I)</th>\n",
       "      <th>BNST (E)</th>\n",
       "      <th>BNST (I)</th>\n",
       "      <th>AH (E)</th>\n",
       "      <th>AH (I)</th>\n",
       "      <th>MeA (E)</th>\n",
       "      <th>MeA (I)</th>\n",
       "      <th>VMH (E)</th>\n",
       "      <th>VMH (I)</th>\n",
       "      <th>NAc (DA)</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.681607</td>\n",
       "      <td>0.894658</td>\n",
       "      <td>-0.888269</td>\n",
       "      <td>-0.843799</td>\n",
       "      <td>-0.340110</td>\n",
       "      <td>-1.034221</td>\n",
       "      <td>-0.819493</td>\n",
       "      <td>-1.648179</td>\n",
       "      <td>-0.129511</td>\n",
       "      <td>-1.166632</td>\n",
       "      <td>-0.772297</td>\n",
       "      <td>-0.993607</td>\n",
       "      <td>-0.569781</td>\n",
       "      <td>-0.558806</td>\n",
       "      <td>-1.871317</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.098652</td>\n",
       "      <td>3.077160</td>\n",
       "      <td>-0.399471</td>\n",
       "      <td>-0.269414</td>\n",
       "      <td>0.300636</td>\n",
       "      <td>-0.750037</td>\n",
       "      <td>-0.568331</td>\n",
       "      <td>-1.223160</td>\n",
       "      <td>0.362066</td>\n",
       "      <td>-0.508080</td>\n",
       "      <td>0.179674</td>\n",
       "      <td>-0.267501</td>\n",
       "      <td>-0.247070</td>\n",
       "      <td>-0.207387</td>\n",
       "      <td>-1.863904</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>4.385353</td>\n",
       "      <td>7.411386</td>\n",
       "      <td>3.316992</td>\n",
       "      <td>3.456766</td>\n",
       "      <td>6.805282</td>\n",
       "      <td>1.133259</td>\n",
       "      <td>4.063267</td>\n",
       "      <td>2.167437</td>\n",
       "      <td>3.659055</td>\n",
       "      <td>2.421159</td>\n",
       "      <td>3.606908</td>\n",
       "      <td>1.320595</td>\n",
       "      <td>3.713601</td>\n",
       "      <td>1.423566</td>\n",
       "      <td>-0.078505</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3.512997</td>\n",
       "      <td>8.480748</td>\n",
       "      <td>4.197532</td>\n",
       "      <td>7.131945</td>\n",
       "      <td>10.586321</td>\n",
       "      <td>2.890310</td>\n",
       "      <td>5.052795</td>\n",
       "      <td>4.965454</td>\n",
       "      <td>4.340566</td>\n",
       "      <td>5.060280</td>\n",
       "      <td>3.577417</td>\n",
       "      <td>2.325631</td>\n",
       "      <td>4.621142</td>\n",
       "      <td>3.081325</td>\n",
       "      <td>0.285582</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.071645</td>\n",
       "      <td>7.150476</td>\n",
       "      <td>3.650279</td>\n",
       "      <td>8.261445</td>\n",
       "      <td>11.256076</td>\n",
       "      <td>3.671176</td>\n",
       "      <td>4.444998</td>\n",
       "      <td>5.760305</td>\n",
       "      <td>4.350413</td>\n",
       "      <td>5.973247</td>\n",
       "      <td>4.558759</td>\n",
       "      <td>3.376738</td>\n",
       "      <td>4.301194</td>\n",
       "      <td>3.936938</td>\n",
       "      <td>0.638092</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21566</th>\n",
       "      <td>0.005666</td>\n",
       "      <td>0.568242</td>\n",
       "      <td>-1.420973</td>\n",
       "      <td>-0.279791</td>\n",
       "      <td>0.111006</td>\n",
       "      <td>-0.122264</td>\n",
       "      <td>-0.404217</td>\n",
       "      <td>-0.332115</td>\n",
       "      <td>-0.131295</td>\n",
       "      <td>-0.161134</td>\n",
       "      <td>-0.342970</td>\n",
       "      <td>-0.396927</td>\n",
       "      <td>1.251091</td>\n",
       "      <td>-0.154511</td>\n",
       "      <td>-0.575245</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21577</th>\n",
       "      <td>-0.002680</td>\n",
       "      <td>0.204383</td>\n",
       "      <td>-1.094548</td>\n",
       "      <td>-0.555328</td>\n",
       "      <td>0.346467</td>\n",
       "      <td>-0.012391</td>\n",
       "      <td>-0.517314</td>\n",
       "      <td>0.148387</td>\n",
       "      <td>0.176850</td>\n",
       "      <td>-0.046641</td>\n",
       "      <td>-0.188476</td>\n",
       "      <td>-0.355746</td>\n",
       "      <td>0.592700</td>\n",
       "      <td>-0.285391</td>\n",
       "      <td>-0.356435</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21588</th>\n",
       "      <td>0.390655</td>\n",
       "      <td>0.053216</td>\n",
       "      <td>-0.545720</td>\n",
       "      <td>-0.542417</td>\n",
       "      <td>0.225190</td>\n",
       "      <td>-0.022971</td>\n",
       "      <td>0.296165</td>\n",
       "      <td>0.047581</td>\n",
       "      <td>-0.224874</td>\n",
       "      <td>-0.155582</td>\n",
       "      <td>-0.019753</td>\n",
       "      <td>-0.381624</td>\n",
       "      <td>0.547959</td>\n",
       "      <td>-0.275207</td>\n",
       "      <td>0.036081</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21599</th>\n",
       "      <td>0.537474</td>\n",
       "      <td>0.268929</td>\n",
       "      <td>-0.350889</td>\n",
       "      <td>-0.610946</td>\n",
       "      <td>0.352079</td>\n",
       "      <td>0.071149</td>\n",
       "      <td>0.383604</td>\n",
       "      <td>0.447735</td>\n",
       "      <td>0.074217</td>\n",
       "      <td>-0.305044</td>\n",
       "      <td>-0.003608</td>\n",
       "      <td>-0.350410</td>\n",
       "      <td>0.426597</td>\n",
       "      <td>-0.423255</td>\n",
       "      <td>-0.151805</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>0.732333</td>\n",
       "      <td>0.284941</td>\n",
       "      <td>-0.290341</td>\n",
       "      <td>-0.240582</td>\n",
       "      <td>0.654424</td>\n",
       "      <td>0.053687</td>\n",
       "      <td>1.225278</td>\n",
       "      <td>0.723362</td>\n",
       "      <td>-0.019893</td>\n",
       "      <td>-0.090577</td>\n",
       "      <td>0.027090</td>\n",
       "      <td>-0.347064</td>\n",
       "      <td>0.389783</td>\n",
       "      <td>-0.358685</td>\n",
       "      <td>-0.519567</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2000 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        PrL (E)   PrL (I)   vLS (E)   vLS (I)    POA (E)   POA (I)  BNST (E)  \\\n",
       "0      0.681607  0.894658 -0.888269 -0.843799  -0.340110 -1.034221 -0.819493   \n",
       "10     1.098652  3.077160 -0.399471 -0.269414   0.300636 -0.750037 -0.568331   \n",
       "21     4.385353  7.411386  3.316992  3.456766   6.805282  1.133259  4.063267   \n",
       "32     3.512997  8.480748  4.197532  7.131945  10.586321  2.890310  5.052795   \n",
       "43     2.071645  7.150476  3.650279  8.261445  11.256076  3.671176  4.444998   \n",
       "...         ...       ...       ...       ...        ...       ...       ...   \n",
       "21566  0.005666  0.568242 -1.420973 -0.279791   0.111006 -0.122264 -0.404217   \n",
       "21577 -0.002680  0.204383 -1.094548 -0.555328   0.346467 -0.012391 -0.517314   \n",
       "21588  0.390655  0.053216 -0.545720 -0.542417   0.225190 -0.022971  0.296165   \n",
       "21599  0.537474  0.268929 -0.350889 -0.610946   0.352079  0.071149  0.383604   \n",
       "21610  0.732333  0.284941 -0.290341 -0.240582   0.654424  0.053687  1.225278   \n",
       "\n",
       "       BNST (I)    AH (E)    AH (I)   MeA (E)   MeA (I)   VMH (E)   VMH (I)  \\\n",
       "0     -1.648179 -0.129511 -1.166632 -0.772297 -0.993607 -0.569781 -0.558806   \n",
       "10    -1.223160  0.362066 -0.508080  0.179674 -0.267501 -0.247070 -0.207387   \n",
       "21     2.167437  3.659055  2.421159  3.606908  1.320595  3.713601  1.423566   \n",
       "32     4.965454  4.340566  5.060280  3.577417  2.325631  4.621142  3.081325   \n",
       "43     5.760305  4.350413  5.973247  4.558759  3.376738  4.301194  3.936938   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "21566 -0.332115 -0.131295 -0.161134 -0.342970 -0.396927  1.251091 -0.154511   \n",
       "21577  0.148387  0.176850 -0.046641 -0.188476 -0.355746  0.592700 -0.285391   \n",
       "21588  0.047581 -0.224874 -0.155582 -0.019753 -0.381624  0.547959 -0.275207   \n",
       "21599  0.447735  0.074217 -0.305044 -0.003608 -0.350410  0.426597 -0.423255   \n",
       "21610  0.723362 -0.019893 -0.090577  0.027090 -0.347064  0.389783 -0.358685   \n",
       "\n",
       "       NAc (DA)  labels  \n",
       "0     -1.871317     5.0  \n",
       "10    -1.863904     6.0  \n",
       "21    -0.078505     6.0  \n",
       "32     0.285582     6.0  \n",
       "43     0.638092     6.0  \n",
       "...         ...     ...  \n",
       "21566 -0.575245     6.0  \n",
       "21577 -0.356435     6.0  \n",
       "21588  0.036081     8.0  \n",
       "21599 -0.151805    10.0  \n",
       "21610 -0.519567    10.0  \n",
       "\n",
       "[2000 rows x 16 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6426961291331363\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "X = test_feat.values[:, :-2]\n",
    "y = test_feat.values[:, -2]\n",
    "\n",
    "inds = np.arange(0, X.shape[0])\n",
    "np.random.shuffle(inds)\n",
    "test_inds, train_inds = inds[:1000], inds[1000:]\n",
    "X_train, X_test, y_train, y_test = X[train_inds], X[test_inds], y[train_inds], y[test_inds]\n",
    "reg = LinearRegression().fit(X_train,y_train)\n",
    "y_out = reg.score(X_test, y_test)\n",
    "print(y_out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up variables with multiregion photom data\n",
    "def get_multi_data(test_data): #(N=20, S=2, P=10, sigma_obs=0.05):\n",
    "\n",
    "    #try setting Y as the NAc DA signal, and using the activity in the other regions to predict (as X)\n",
    "    Y = test_data['NAc (DA)'].to_numpy()\n",
    "    X = test_data.iloc[:,:14].to_numpy()\n",
    "\n",
    "    Y -= jnp.mean(Y)\n",
    "    Y_std = jnp.std(Y)\n",
    "    print(Y_std)\n",
    "\n",
    "    return X, Y / Y_std, 0, 1.0 / Y_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_multi(args,test_data):\n",
    "    X, Y, expected_thetas, expected_pairwise = get_multi_data(test_data)\n",
    "\n",
    "    # setup hyperparameters\n",
    "    hypers = {\n",
    "        \"expected_sparsity\": max(1.0, args.num_dimensions / 10),\n",
    "        \"alpha1\": 3.0,\n",
    "        \"beta1\": 1.0,\n",
    "        \"alpha2\": 3.0,\n",
    "        \"beta2\": 1.0,\n",
    "        \"alpha3\": 1.0,\n",
    "        \"c\": 1.0,\n",
    "    }\n",
    "\n",
    "    # do inference\n",
    "    rng_key = random.PRNGKey(0)\n",
    "    samples = run_inference(model, args, rng_key, X, Y, hypers)\n",
    "\n",
    "    # compute the mean and square root variance of each coefficient theta_i\n",
    "    means, stds = vmap(lambda dim: analyze_dimension(samples, X, Y, dim, hypers))(\n",
    "        jnp.arange(args.num_dimensions)\n",
    "    )\n",
    "\n",
    "    print(\n",
    "        \"Coefficients theta_1 to theta_%d used to generate the data:\"\n",
    "        % args.active_dimensions,\n",
    "        expected_thetas,\n",
    "    )\n",
    "    print(\n",
    "        \"The single quadratic coefficient theta_{1,2} used to generate the data:\",\n",
    "        expected_pairwise,\n",
    "    )\n",
    "    active_dimensions = []\n",
    "\n",
    "    for dim, (mean, std) in enumerate(zip(means, stds)):\n",
    "        # we mark the dimension as inactive if the interval [mean - 3 * std, mean + 3 * std] contains zero\n",
    "        lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "        inactive = \"inactive\" if lower < 0.0 and upper > 0.0 else \"active\"\n",
    "        if inactive == \"active\":\n",
    "            active_dimensions.append(dim)\n",
    "        print(\n",
    "            \"[dimension %02d/%02d]  %s:\\t%.2e +- %.2e\"\n",
    "            % (dim + 1, args.num_dimensions, inactive, mean, std)\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        \"Identified a total of %d active dimensions; expected %d.\"\n",
    "        % (len(active_dimensions), args.active_dimensions)\n",
    "    )\n",
    "\n",
    "    # Compute the mean and square root variance of coefficients theta_ij for i,j active dimensions.\n",
    "    # Note that the resulting numbers are only meaningful for i != j.\n",
    "    if len(active_dimensions) > 0:\n",
    "        dim_pairs = jnp.array(\n",
    "            list(itertools.product(active_dimensions, active_dimensions))\n",
    "        )\n",
    "        means, stds = vmap(\n",
    "            lambda dim_pair: analyze_pair_of_dimensions(\n",
    "                samples, X, Y, dim_pair[0], dim_pair[1], hypers\n",
    "            )\n",
    "        )(dim_pairs)\n",
    "        for dim_pair, mean, std in zip(dim_pairs, means, stds):\n",
    "            dim1, dim2 = dim_pair\n",
    "            if dim1 >= dim2:\n",
    "                continue\n",
    "            lower, upper = mean - 3.0 * std, mean + 3.0 * std\n",
    "            if not (lower < 0.0 and upper > 0.0):\n",
    "                format_str = \"Identified pairwise interaction between dimensions %d and %d: %.2e +- %.2e\"\n",
    "                print(format_str % (dim1 + 1, dim2 + 1, mean, std))\n",
    "\n",
    "        # Draw a single sample of coefficients theta from the posterior, where we return all singleton\n",
    "        # coefficients theta_i and pairwise coefficients theta_ij for i, j active dimensions. We use the\n",
    "        # final MCMC sample obtained from the HMC sampler.\n",
    "\n",
    "        # FOR THE PROJECT\n",
    "        # Modify this function to produce valid posterior samples of theta from all the tau samples from the MCMC step\n",
    "        # but restrict yourself to the active direct and pairwise interaction terms\n",
    "        thetas = sample_theta_space(\n",
    "            X,\n",
    "            Y,\n",
    "            active_dimensions,\n",
    "            samples[\"msq\"][-1],\n",
    "            samples[\"lambda\"][-1],\n",
    "            samples[\"eta1\"][-1],\n",
    "            samples[\"xisq\"][-1],\n",
    "            hypers[\"c\"],\n",
    "            samples[\"sigma\"][-1],\n",
    "        )\n",
    "        print(\"Single posterior sample theta:\\n\", thetas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.98603415\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    assert numpyro.__version__.startswith(\"0.11.0\")\n",
    "    parser = argparse.ArgumentParser(description=\"Gaussian Process example\")\n",
    "    parser.add_argument(\"-n\", \"--num-samples\", nargs=\"?\", default=1000, type=int)\n",
    "    parser.add_argument(\"--num-warmup\", nargs=\"?\", default=500, type=int)\n",
    "    parser.add_argument(\"--num-chains\", nargs=\"?\", default=1, type=int)\n",
    "    parser.add_argument(\"--num-data\", nargs=\"?\", default=100, type=int)\n",
    "    parser.add_argument(\"--num-dimensions\", nargs=\"?\", default=20, type=int)\n",
    "    parser.add_argument(\"--active-dimensions\", nargs=\"?\", default=14, type=int)\n",
    "    parser.add_argument(\"--device\", default=\"cpu\", type=str, help='use \"cpu\" or \"gpu\".')\n",
    "    #ipy seems to want a \"-f\" arg, so this is my fix atm\n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    numpyro.set_platform(args.device)\n",
    "    numpyro.set_host_device_count(args.num_chains)\n",
    "\n",
    "    main_multi(args,test_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
